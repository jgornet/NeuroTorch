{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "# Notebook auto reloads code.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroTorch Tutorial\n",
    "\n",
    "**NeuroTorch** is a framework for reconstructing neuronal morphology from\n",
    "optical microscopy images. It interfaces PyTorch with different\n",
    "automated neuron tracing algorithms for fast, accurate, scalable\n",
    "neuronal reconstructions. It uses deep learning to generate an initial\n",
    "segmentation of neurons in optical microscopy images. This\n",
    "segmentation is then traced using various automated neuron tracing\n",
    "algorithms to convert the segmentation into an SWC file—the most\n",
    "common neuronal morphology file format. NeuroTorch is designed with\n",
    "scalability in mind and can handle teravoxel-sized images.\n",
    "\n",
    "This IPython notebook will outline a brief tutorial for using NeuroTorch\n",
    "to train and predict on image volume datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating image datasets\n",
    "\n",
    "One of NeuroTorch’s key features is its dynamic approach to volumetric datasets, which allows it to handle teravoxel-sized images without worrying about memory concerns and efficiency. Everything is loaded just-in-time based on when it is needed or expected to be needed. To load an image dataset, we need\n",
    "to specify the voxel coordinates of each image file as shown in files `inputs_spec.json` and `labels_spec.json`.\n",
    "\n",
    "### `inputs_spec.json`\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "\t\"filename\" : \"inputs.tif\",\n",
    "\t\"bounding_box\" : [[0, 0, 0], [1024, 512, 50]]\n",
    "    },\n",
    "    {\n",
    "\t\"filename\" : \"inputs.tif\",\n",
    "\t\"bounding_box\" : [[0, 0, 50], [1024, 512, 100]]\n",
    "    }\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "### `labels_spec.json`\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "\t\"filename\" : \"labels.tif\",\n",
    "\t\"bounding_box\" : [[0, 0, 0], [1024, 512, 50]]\n",
    "    },\n",
    "    {\n",
    "\t\"filename\" : \"labels.tif\",\n",
    "\t\"bounding_box\" : [[0, 0, 50], [1024, 512, 100]]\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading image datasets\n",
    "\n",
    "Now that the image datasets for the inputs and labels have been specified,\n",
    "these datasets can be loaded with NeuroTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurotorch.datasets.specification import JsonSpec\n",
    "import os\n",
    "\n",
    "IMAGE_PATH = '/data/gornet/annotated-neurons'\n",
    "\n",
    "json_spec = JsonSpec() # Initialize the JSON specification\n",
    "\n",
    "# Create a dataset containing the inputs\n",
    "inputs = json_spec.open(os.path.join(IMAGE_PATH,\n",
    "                                     \"input_spec.json\")) \n",
    "\n",
    "# Create a dataset containing the labels\n",
    "labels = json_spec.open(os.path.join(IMAGE_PATH,\n",
    "                                     \"label_spec.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurotorch.datasets.specification import JsonSpec\n",
    "import os\n",
    "\n",
    "IMAGE_PATH = '../src/tests/images/'\n",
    "\n",
    "json_spec = JsonSpec() # Initialize the JSON specification\n",
    "\n",
    "# Create a dataset containing the inputs\n",
    "inputs = json_spec.open(os.path.join(IMAGE_PATH,\n",
    "                                     \"inputs_spec.json\")) \n",
    "\n",
    "# Create a dataset containing the labels\n",
    "labels = json_spec.open(os.path.join(IMAGE_PATH,\n",
    "                                     \"labels_spec.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurotorch.datasets.dataset import AlignedVolume\n",
    "\n",
    "volume = AlignedVolume([inputs, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the image datasets\n",
    "\n",
    "To train a neural network using these image datasets, load the \n",
    "neural network architecture and initialize a `Trainer`. To save\n",
    "training checkpoints, add a `CheckpointWriter` to the `Trainer` object.\n",
    "Lastly, call the `Trainer` object to run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from neurotorch.core.trainer import Trainer\n",
    "from neurotorch.training.logging import LossWriter, ImageWriter\n",
    "from neurotorch.training.checkpoint import CheckpointWriter\n",
    "from neurotorch.nets.RSLSTMUnet import RSUNet\n",
    "\n",
    "net = RSUNet([4, 16], [128], (16, 64, 64)) # Initialize the U-Net architecture\n",
    "\n",
    "# Setup the trainer\n",
    "trainer = Trainer(net, volume, max_epochs=10,\n",
    "                  gpu_device=1)\n",
    "\n",
    "# Setup the trainer the add a checkpoint every 500 epochs\n",
    "trainer = CheckpointWriter(trainer, checkpoint_dir='BMEN4000',\n",
    "                           checkpoint_period=2000)\n",
    "trainer = LossWriter(trainer, \".\", \"BMEN4000\")\n",
    "trainer = ImageWriter(trainer, \".\", \"BMEN4000\")\n",
    "\n",
    "trainer.run_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using NeuroTorch\n",
    "\n",
    "Once training has completed, we can use the training checkpoints\n",
    "to predict on image datasets. We first have to \n",
    "load the neural network architecture and image volume.\n",
    "We then have to initialize a `Predictor` object and an output volume.\n",
    "Once these have been specified, we can begin prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurotorch.nets.RSLSTMUnet import RSUNet\n",
    "from neurotorch.core.predictor import Predictor\n",
    "from neurotorch.datasets.filetypes import TiffVolume\n",
    "from neurotorch.datasets.dataset import Array\n",
    "from neurotorch.datasets.datatypes import (BoundingBox, Vector)\n",
    "import numpy as np\n",
    "import tifffile as tif\n",
    "import os\n",
    "\n",
    "IMAGE_PATH = 'tests/images/'\n",
    "\n",
    "net = RSUNet([4, 16], [128], (8, 32, 32)) # Initialize the U-Net architecture\n",
    "\n",
    "checkpoint = './checkpoints/best.ckpt-39' # Specify the checkpoint path\n",
    "\n",
    "with TiffVolume(os.path.join(IMAGE_PATH,\n",
    "                             \"inputs.tif\"),\n",
    "                BoundingBox(Vector(0, 0, 0),\n",
    "                            Vector(1024, 512, 50)),\n",
    "                iteration_size=BoundingBox(Vector(0, 0, 0),\n",
    "                                          Vector(32, 32, 8)),\n",
    "                stride=Vector(4, 16, 16)) as inputs:\n",
    "    predictor = Predictor(net, checkpoint, gpu_device=0)\n",
    "\n",
    "    output_volume = Array(np.zeros(inputs.getBoundingBox()\n",
    "                                   .getNumpyDim(), dtype=np.float32))\n",
    "\n",
    "    predictor.run(inputs, output_volume, batch_size=16)\n",
    "\n",
    "    tif.imsave(\"test_prediction.tif\",\n",
    "               output_volume.getArray().astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the prediction\n",
    "\n",
    "Predictions are output in logits form. To map this to a\n",
    "probability distribution, we need to apply a sigmoid function\n",
    "to the prediction. We can then evaluate the prediction and \n",
    "ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sigmoid function\n",
    "probability_map = 1/(1+np.exp(-output_volume.getArray()))\n",
    "\n",
    "# Plot prediction and ground-truth\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Prediction')\n",
    "plt.imshow(np.amax(output_volume.getArray(), axis=0))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Ground-Truth')\n",
    "plt.imshow(labels.get(\n",
    "    BoundingBox(Vector(0, 0, 0),\n",
    "                Vector(1024, 512, 50))).getArray()[25],\n",
    "           cmap='gray'\n",
    "          )\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
